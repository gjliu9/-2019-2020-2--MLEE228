{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "NUM_EPOCHS = 5\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,    # 输入通道数\n",
    "                out_channels=16,  # 输出通道数\n",
    "                kernel_size=5,    # 卷积核的尺寸是(5,5)\n",
    "                stride=1,         # 步长为1\n",
    "                padding=2         # 零填充保持图片宽高不变,padding = (kernel_size - stride) / 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),   # 批量标准化\n",
    "            nn.MaxPool2d(kernel_size=2)   # 最大池化层，卷积核尺寸是(2,2)，该层输出的样本尺寸:16x14x14\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2)              # 该层输出样本尺寸：32x7x7\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # 全连接层输出10个类别\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #使用GPU\n",
    "MNIST_model = SimpleNet().to(DEVICE)\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "optimizer = torch.optim.Adam(MNIST_model.parameters(), lr=LR)  # 定义优化器Adam\n",
    "loss_func = nn.CrossEntropyLoss()                              # 定义交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:12<00:00, 23.17it/s]\n",
      "  1%|▏         | 4/300 [00:00<00:09, 30.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59893/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9903/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:12<00:00, 23.16it/s]\n",
      "  1%|          | 2/300 [00:00<00:16, 18.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59829/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9895/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:13<00:00, 22.92it/s]\n",
      "  1%|          | 3/300 [00:00<00:12, 23.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59872/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9890/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:12<00:00, 23.99it/s]\n",
      "  1%|          | 3/300 [00:00<00:13, 22.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59993/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9919/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:11<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59998/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9919/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        # TODO:forward + backward + optimize\n",
    "        images,labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = MNIST_model(images)\n",
    "        loss = loss_func(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image,label in train_loader:\n",
    "            image,label = image.to(DEVICE),label.to(DEVICE)\n",
    "            output = MNIST_model(image)\n",
    "            train_loss += loss_func(output,label)\n",
    "            pred = output.max(1,keepdim=True)[1]\n",
    "            train_correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image,label in test_loader:\n",
    "            image,label = image.to(DEVICE),label.to(DEVICE)\n",
    "            output = MNIST_model(image)\n",
    "            test_loss += loss_func(output,label)\n",
    "            pred = output.max(1,keepdim=True)[1]\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('In train_set: \\n Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('In test_set: \\n Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_set: \n",
      " Average loss: 0.0000, Accuracy: 59998/60000 (100%)\n",
      "\n",
      "In test_set: \n",
      " Average loss: 0.0002, Accuracy: 9919/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('In train_set: \\n Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    train_loss, train_correct, len(train_loader.dataset),\n",
    "    100. * train_correct / len(train_loader.dataset)))\n",
    "print('In test_set: \\n Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
